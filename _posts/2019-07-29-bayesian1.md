---
title: 'Bayesian Methods and Molecular Modelling 1'
date: 2019-07-29
permalink: /posts/2019/07/bayesian1/
tags:
  - molecularModelling
  - scientificComputing
---
(Last updated: 2019-07-30). This is an ongoing post as I work through a tutorial I found.

# Introduction
Within the world of molecular modelling, a very exciting group is doing some 
interesting work. They are the 
[OpenForceField consortium](https://openforcefield.org/).
Looking through one of their 
[workshops](https://openforcefield.org/news/jan-2019-meeting-agenda/),
they draw some interesting analogies between 
*developing force fields and bayesian inference*.
As someone *transitioning from molecular modelling to data science*, this is 
certainly an interesting intersection.

## A big question
*Given some thermodynamic/QM data, how can we best parametrize force fields?*

Let's step back a bit and make it sound more statistics-y.

*Given some energetic data, how can we best model the energy of a system? 
What is the best distribution/equation to model the energy (aka force field)? 
And what are the best distributions to describe the parameters of the model?
From these fitted/parametrized models, can we interpret anything from them?
Can we sample/simulate anything from them?*

# Learning some Bayesian methods
Here's where I stumble through miscellaneous talks, lectures, publications,
and tutorials to try to piece together bits of this field.
I found a particularly interesting tutorial by [Chris Fonnesbeck](https://github.com/fonnesbeck/mcmc_pydata_london_2019)

**Important**: I am *really* hoping someone out there is reading this and 
will correct me where I am wrong. I've learned solo-learning 
can be trial-and-error, and having external input can be very invaluable
for guiding education.

## Notebook 1: Introduction to PyMC3
Their example is studying radon levels (actually, log(radon)) in households, 
developing a model, 
and trying to identify how many households are greather than a threshold.

* We are interested in the **posterior distribution** given the data
* To develop a **sampling distribution** for the data, 
they use a *normal distribution*, which requires parameters for $\mu$ and $\sigma$,
and also is based on an **observed distribution** (the gathered data)
* To develop a **model or distribution for $\mu$**, they first approximate/guess
with a *normal distribution* (it's okay if the log radon is negative)
* To develop a **model or distribution for $\sigma$**, they first approximate/guess
with a *uniform distribution* (standard deviations and variances shouldn't 
be negative)

In actuality, first the models for $\mu$ and $\sigma$ came first,
THEN the sampling distribution for the data was built off the $\mu$ and $\sigma$ 
distributions in addition to the observed data

The overarching model was fit using a **Markov chain Monte Carlo** (MCMC) method.

After fitting the model we can observe some probabilities about a point estimate,
$\mu$:

* Probability that the mean level of radon is *above* a threshold
* Probability that the mean level of radon is *below* a threshold

### Translation to molecular modelling
* Our **posterior distribution** is the force field (potential energy function)
given some observable. Analogously, this was the posterior distribution of radon
levels given the observed data.

* In our molecular model, let's say we just model the force field 
as harmonic springs 
![Harmonic spring](/images/mm/harmonicspring.png). Here, the two parameters
are $k$ and $r_0$
* In the radon model, we modelled with a normal distribution. There, the two 
parameters were $\mu$ and $\sigma$
* We used normal distributions for $\mu$ and $\sigma$. For $k$ and $r_0$, we know
they should be non-negative (force constants and equilibrium lengths, 
respectively, are physically and generally non-negative), but we could
use some sort of distribution that won't get us negatives

At this point in both cases:

* We have an equation that expresses either our energy 
or the radon level (force field, posterior distribution). 
* We have also identified the associated parameters
in these equations ($k$ & $r_0$, $\mu$ & $\sigma$). 
* For these parameters, we are estimating/approximating them via some 
other equation or distribution (normal or uniform)

The next step is to fit the model using some sort of sampling method. 
Molecular modellers would call this parametrization.
In layman's terms, I think this means some sophisticated guess-and-check or
numerical optimization method to get the parameters 
that well-describe the observed data.

Now we have our parameters, so we can sample from this model or distribution 
(simulating, sampling), and see how well the simulated/sampled data
recreates the generated data.

We can interpret these models: make some statements about the 
mean radon model or make some statements about the equilibrium bond length

Lastly, sensitivity analysis means messing with the underlying assumptions -
what if we didn't use normal/uniform distributions to describe [$k$, $r_0$] or
[$\mu$, $\sigma$]? I think you could go one step further and mess
with the underlying model, what if the radon level wasn't modelled as a 
normal distribution? 
What if the force field wasn't modelled as harmonic spring bonds?

I would hazard a guess and say not to try radically different models if they
intuitively don't really describe the data we are interested in modelling -
modelling a force field (energy) as a purely random distribution would probably 
be a physically-unrealistic model and would not describe your data well.


## Notebook 2: Markov chain Monte Carlo

This was a lot more than notebook 1. On a completely different note, 
there's so many parallels to molecular modelling here, and I keep thinking
of everything in terms of molecular modelling and I'm not sure if that's
hindering or facilitating my education. I'm going to unwind some thoughts here.

Once again, Bayesian methods are used to estimate parameters of a model.
If your model is a normal distribution, your parameters are $\mu$ and $\sigma$.
If your model is a line, your parameters are slope and intercept
If your model is a harmonic spring, your parameters are force constant 
and equilibrium bond length.

Bayes' formula is used to estimate the probability of certain parameters given
some observed data.

![Bayes' theorem](/images/bayes.png)

The denominator, $P(y)$ is called a **normalization constant** or 
**marginal likelihood**. Okay, well how do you calculate that?

![Bayes' thm with integral](/images/bayes_int.png)

To compute the marginal likelihood (probability of one variable), 
we have to compute this integral of the 
joint probability distribution (probabilities of two variables)
along the other variable.

Here's the first analogy to statistical mechanics:

![Canonical energy](/images/mm/canonical_energy_prob.png)

Where $\epsilon$ is the energy of state, $k_B$ is Boltzmann's constant,
$T$ is temperature, and $Q$ is our normalization constant
(in fancy stat mech terms, this is the **canonical partition function**).
Except this probably isn't as noteworthy as it seems, since probabilities,
in general, are just numbers of desired outcomes divided by all possible outcomes.

Back to Bayesian, this marginal likelihood integral is analytically hard to 
compute, so we have to use some numerical sampling methods. In molecular modelling,
we are also trying to compute an integral over *phase space* (the 6N distribution 
over positions and velocities, but really just 3N positions because the velocity
component is *a priori* known based on temperature). In both methods, 
we can employ **Monte Carlo** methods. In Bayesian, this is Markov chain Monte Carlo.

### Side note: Monte Carlo integration
Let's say you wanted to integrate $f(x)dx$. Assuming $f(x)$ is complicated
and can't be solved analytically, you use a numerical method like 
Riemann sums, Simpson's Rule, Trapezoidal Rule, etc. The summary of these
numerical methods is you (uniformly) pick a bunch of $x$, compute $f(x)$, and 
draw a bunch of rectangles whose height is $f(x)$ and length $x_{i+1} -x_i$ 
(otherwise known as $dx$),
then sum up all those areas. 

What if you didn't pick $x$ uniformly?
If you randomly picked $x$ (and adjusted the area summation to account
for the non-uniform $dx$), you would be implementing a crude
Monte Carlo integration.

To sum up Monte Carlo, pick random inputs, evaluate the function at 
those inputs, compute area, and sum.

### Back to the notebook
**Markov chains** essentially mean that the probability of visiting a 
future state is dependent on what your current state, but
not any other state in history. In the Monte Carlo scheme, 
this means, while you are "randomly" generating 
inputs, your next input is somewhat dependent on your current one.

Some of the underlying theory says you can use Bayes' theorem (again,
but applied to the Markov chain, not how we're obtaining model parameters), to
demonstrate a concept known as **detailed balance** that means you have 
balanced movement through phase space (this is a princple that holds for 
broader statistics in addition to molecular modelling Monte Carlo methods).

Okay, so if our next input depends on our current input, what is the critieria
or algorithm we use to pick the next input in this integration scheme?

One algorithm is **Metropolis-Hastings** where you accept or reject
based on the ratio of probabilities of new-input to current-input. 
This means comparing $P(\theta')$ to $P(\theta)$, which I think
means referencing our posterior distribution ($P\theta|y$).

In molecular modelling, we relate ratios to exponentials of energies 
so that our acceptance
ratio simplifies to:
![metropolis hastings energy](/images/mm/metropolis_hastings_energy.png)

### Checkpoint: maybe summarizing things so far

* We want to model some sort of observable, which requires parameters
* We want to get the observable's **posterior distribution**, 
which tells the probability distribution of certain 
parameters given the observable data
* We have Bayes' theorem that formalizes some relationships 
![Bayes](/images/bayes.png)
* We have the **likelihood** $P(y|\theta)$, which is 
an equation/distribution that depends on our parameters. From notebook1,
this is a model/distribution we specify beforehand, with parameters we are 
trying to find.
* We have the **prior** $P(\theta)$, which
is an equation/distribution of our parameters. From notebook1,
this is a model/distribution we specify beforehand, with parameters we guessed.
* We have our **marginal likelihood** $P(y)$, which is re-written as 
an integral of $P(y|\theta)P(\theta)d\theta$, which is an integral
of our likelihood and prior we just outlined.
* To evaluate this integral, we use a **Markov chain Monte Carlo** method
to pick different values of $\theta$, evaluate the product of likelihood and prior, 
then compute the summation. This integration method requires lots of 
iterations/sampling (this sampling is NOT sampling the observed data, but sampling
different $\theta$ parameters).
    * This integral is really difficult to comprehend in my opinion.
    * You are computing an integral to end up with a posterior distribution
    * But to compute the integral iteratively and numerically, you need to
    continuously re-evaluate the posterior distribution to choose moves based on 
    **Metropolis-Hastings**.
* How we efficiently calculate this integral is where the Bayesian inference
methods get interesting from a computational perspective.

I think writing some of this out maybe helped my own understanding. Again, if 
someone out there is reading this and deems me incorrect, PLEASE contact me - 
I'd love to learn this properly. For now, I'm going to take a break
and return to this notebook 2 later - we're going to be hitting 
some crazy similarities to molecular simulation.
