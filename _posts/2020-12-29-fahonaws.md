---
title: FoldingAtHome on AWS
date: 2020-12-29
permalink: /posts/2020/12/fahonaws
tags:
    - personal
    - molecularmodeling
    - datascience
---
# Accessing FoldingAtHome data on AWS


Some F@H data is [freely accessible on AWS](https://registry.opendata.aws/foldingathome-covid19/).
This will be a relatively short post on accessing and navigating the data on AWS.

If you regularly use AWS, this will be nothing new. 
If you're a grad student who has only ever navigated local file directories or used `scp`/`rsync`/`ssh` to interact with remote clusters, this might be your first time interacting with files on AWS S3.

The python environment is fairly straightforward analytical environment, but with s3fs, boto3, and botocore to interact with files on S3

`conda create -n fahaws python=3.7 pandas s3fs jupyter ipykernel -c conda-forge -yq`

(Active environment)

`python -m pip install boto3 botocore`

## The AWS CLI

The tools to navigate files within AWS directories follow that of unix-like systems.
[AWS CLI installation](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html).

`aws s3 ls s3://fah-public-data-covid19-absolute-free-energy/ --no-sign-request` to list files within this particular S3 bucket. The no sign request flag at the end helps us bypass the need for any credentials.

You can read from stdout or pipe the output to a textfile, but this will be your bread and butter for wading through terabytes and terabytes of F@H data.

As of this post (Dec 2020), looks like the files in `free_energy_data/` have been last updated end of Sept 2020

## Summary of free energy results data

Fortunately, loading remote files via pandas is a common task, so there are convenient functions.
Loading a dataframe over S3 is just like loading a dataframe locally (note the S3 string syntax)

The column `febkT` looks like the binding free energies in units of $k_B T$ (multiply by Boltzmann's constant and temperature to get energies in kJ or kcal).
It's worth mentioning that the value of the binding free energy is not as helpful as the _relative_ binding free energy to find the best binder of the bunch (how do these free energies compare against each other?)


```python
import pandas as pd
```


```python
df = pd.read_pickle("s3://fah-public-data-covid19-absolute-free-energy/free_energy_data/results.pkl")
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>dataset</th>
      <th>fah</th>
      <th>identity</th>
      <th>receptor</th>
      <th>score</th>
      <th>febkT</th>
      <th>error</th>
      <th>ns_RL</th>
      <th>ns_L</th>
      <th>wl_RL</th>
      <th>L_error</th>
      <th>RL_error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1155</th>
      <td>MS0323_v3</td>
      <td>PROJ14822/RUN258</td>
      <td>DAR-DIA-43a-5</td>
      <td>protein-0387.pdb</td>
      <td>-5.201610</td>
      <td>-25.546943</td>
      <td>3.773523</td>
      <td>[131, 89, 74, 113, 80]</td>
      <td>[450, 490, 540, 410, 620]</td>
      <td>[0.18446, 0.14757, 0.18446, 0.18446, 0.18446]</td>
      <td>0.116912</td>
      <td>3.280887</td>
    </tr>
    <tr>
      <th>609</th>
      <td>MS0326_v3</td>
      <td>PROJ14823/RUN1202</td>
      <td>MUS-SCH-c2f-13</td>
      <td>Mpro-x0107-protein.pdb</td>
      <td>-9.550890</td>
      <td>-25.259420</td>
      <td>22.776358</td>
      <td>[121, 138, 96, 16, 5]</td>
      <td>[200, 200, 200, 200, 200]</td>
      <td>[0.18446, 0.18446, 0.23058, 0.23058, 0.23058]</td>
      <td>16.216396</td>
      <td>0.109175</td>
    </tr>
    <tr>
      <th>759</th>
      <td>MS0331_v3</td>
      <td>PROJ14825/RUN685</td>
      <td>MAK-UNK-129-18</td>
      <td>Mpro-x0107_0.pdb</td>
      <td>-8.425830</td>
      <td>-24.789359</td>
      <td>18.021078</td>
      <td>[58, 68, 5, 7]</td>
      <td>[200]</td>
      <td>[0.37782, 0.30226, 0.9224, 0.59034]</td>
      <td>0.000000</td>
      <td>9.238496</td>
    </tr>
    <tr>
      <th>615</th>
      <td>MS0326_v3</td>
      <td>PROJ14823/RUN2911</td>
      <td>√ÅLV-UNI-7ff-30</td>
      <td>Mpro-x0540-protein.pdb</td>
      <td>-2.774634</td>
      <td>-24.447756</td>
      <td>6.605737</td>
      <td>[174, 124, 70]</td>
      <td>[200, 200, 200, 200, 200]</td>
      <td>[0.14757, 0.14757, 0.18446]</td>
      <td>0.042010</td>
      <td>5.184169</td>
    </tr>
    <tr>
      <th>1086</th>
      <td>MS0326_v3</td>
      <td>PROJ14823/RUN2580</td>
      <td>SEL-UNI-842-3</td>
      <td>Mpro-x0397-protein.pdb</td>
      <td>-4.474095</td>
      <td>-23.705301</td>
      <td>1.248983</td>
      <td>[166, 134, 45]</td>
      <td>[200, 200, 200, 200, 200]</td>
      <td>[0.18015, 0.22519, 0.35183]</td>
      <td>0.212546</td>
      <td>2.529874</td>
    </tr>
  </tbody>
</table>
</div>



## Some code to iterate through these buckets

Pythonically, we can build some S3 code to list each object in this S3 bucket.


```python
import boto3
from botocore import UNSIGNED
from botocore.client import Config

s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))
s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))

bucket_name = "fah-public-data-covid19-absolute-free-energy"
bucket = s3.Bucket(bucket_name)
```

This S3 bucket is very large -- all the simulation inputs, trajectories, and outputs are in here, so it will take a while to enumerate every object.
Instead, we'll just make a generator and pull out a single item for proof-of-concept.


```python
paginator = s3_client.get_paginator('list_objects_v2')
pages = paginator.paginate(Bucket=bucket_name)
```


```python
def page_iterator(pages):
    for page in pages:
        for item in page['Contents']:
            yield item['Key']
```


```python
all_objects = page_iterator(pages)
```


```python
next(all_objects)
```




    'PROJ14377/RUN0/CLONE0/frame0.tpr'



And if you wanted to, you could layer a filter over the generator to impose some logic like filtering for the top-level directories


```python
first_level_dirs = filter(lambda x: x.count('/')==1, all_objects)
```


```python

```

Notebook itself can be found [here](../files/notebooks/2020-12-29-fahonaws.ipynb)
